{"cells":[{"metadata":{"id":"EI1he77yvm16"},"cell_type":"markdown","source":"<h2 align=center> Facial Expression Recognition with Keras</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tarfile\nfname = '../input/challenges-in-representation-learning-facial-expression-recognition-challenge/fer2013.tar.gz'\nif fname.endswith(\"tar.gz\"):\n    tar = tarfile.open(fname , \"r:gz\")\n    tar.extractall()\n    tar.close()\nelif fname.endswith(\"tar\"):\n    tar = tarfile.open(fname, \"r:\")\n    tar.extractall()\n    tar.close()","execution_count":null,"outputs":[]},{"metadata":{"id":"rVpjBpXfvm18"},"cell_type":"markdown","source":" "},{"metadata":{"id":"S-bjocpuvm19"},"cell_type":"markdown","source":"### Task 1: Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install utils\n!pip install livelossplot\n","execution_count":null,"outputs":[]},{"metadata":{"id":"wvGxjjeV-9Ls","trusted":true},"cell_type":"code","source":"import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport utils\nimport pandas as pd\nimport os\n%matplotlib inline\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D\nfrom tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.utils import plot_model\nfrom keras.utils import np_utils\n\nfrom IPython.display import SVG, Image\n# from livelossplot import PlotLossesTensorFlowKeras\nimport tensorflow as tf\nprint(\"Tensorflow version:\", tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## append two csv file into one file for trainning"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('./fer2013/fer2013.csv')\ndf2 = pd.read_csv('../input/challenges-in-representation-learning-facial-expression-recognition-challenge/train.csv')\ndf.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Usage'] , axis =1, inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"number of images is : \" ,df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"number of images is : \" ,df2.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of labels \n\ndf['emotion'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## summary about data \n\n1- number of images is 35887\n\n2- number of label is 7 label from(0 - 6)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"label =[]\ndata  =[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## convert pixels into two categories\n# read first file\n\nimport csv\n\n\nflag = True\n\nwith open (\"./fer2013/fer2013.csv\", 'r') as f:\n    \n    reader = csv.reader (f, delimiter=',')\n    for row in reader:\n    # skip first row\n        if flag == True:\n            flag = False\n\n        else:\n            label.append(row[0])\n            data.append([int(p) for p in row[1].split()])\n \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#read second file\n\nimport csv\n\n\nflag = True\n\nwith open (\"../input/challenges-in-representation-learning-facial-expression-recognition-challenge/train.csv\", 'r') as f:\n    \n    reader = csv.reader (f, delimiter=',')\n    for row in reader:\n    # skip first row\n        if flag == True:\n            flag = False\n\n        else:\n            label.append(row[0])\n            data.append([int(p) for p in row[1].split()])\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.array(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label = np.array(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## reshape image into 48*48\nsample,w = data.shape\n\ndata = data.reshape(sample , 48 ,48 ,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"1KDCNnFmvm2I"},"cell_type":"markdown","source":"### Task 2: Plot Sample Image"},{"metadata":{"id":"TalL_1Qr-9Qz","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"x_train is : \" , X_train.shape)\nprint(\"y_train is : \" , y_train.shape)\nprint(\"X_test is : \" , X_test.shape)\nprint(\"y_test is : \" , y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## scale image \nX_train = X_train/255.0\nX_test  = X_test/255.0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutr']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# return class name\ndef class_name(label):\n    return class_names[label]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.color import rgb2gray\n\n\nplt.figure(figsize = (20,10))\nfor i in range(20):\n    plt.subplot(4,5,i+1)\n    index = np.random.choice(X_train.shape[0] , 1 , replace =False)\n    img = X_train[index].reshape(48,48)\n    img = rgb2gray(img)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    plt.xlabel(class_name(int(y_train[index])))","execution_count":null,"outputs":[]},{"metadata":{"id":"VMoJgN1Evm2W"},"cell_type":"markdown","source":"### Task 3: Generate Training and Validation Batches"},{"metadata":{"id":"iri8ehFw-9Tj","trusted":true},"cell_type":"code","source":"## hyperparameter \n\nwidth  = 48\nheight = 48\n\nbatch_size = 64\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"BZMLrjZQvm2d"},"cell_type":"markdown","source":" "},{"metadata":{"id":"xD7m30ssvm2e"},"cell_type":"markdown","source":"### Task 4: Create CNN Model"},{"metadata":{"id":"a99sH7whvm2g"},"cell_type":"markdown","source":"![](model.png)\nInspired by Goodfellow, I.J., et.al. (2013). Challenged in representation learning: A report of three machine learning contests. *Neural Networks*, 64, 59-63. [doi:10.1016/j.neunet.2014.09.005](https://arxiv.org/pdf/1307.0414.pdf)"},{"metadata":{"id":"v4b5Aidevm2h","trusted":true},"cell_type":"code","source":"model = Sequential()\n\n#first convd\nmodel.add(Conv2D(64 , 3 , activation = 'relu' , padding ='same' , input_shape = (48,48,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size = 2))\nmodel.add(Dropout(0.25))\n    \n#second convd\nmodel.add(Conv2D(128 , 5 , activation = 'relu' , padding ='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size = 2))\nmodel.add(Dropout(0.25))\n\n#third convd\nmodel.add(Conv2D(512 , 3 , activation = 'relu' , padding ='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size = 2))\nmodel.add(Dropout(0.25))\n\n#fourth convd\nmodel.add(Conv2D(512 , 3 , activation = 'relu' , padding ='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size = 2))\nmodel.add(Dropout(0.25))\n\n#flatten\n\nmodel.add(Flatten())\n\n#first Dense\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n#second Dense\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n#output\nmodel.add(Dense(7 , activation = 'softmax'))\n\n\nmodel.compile(\n        loss = 'categorical_crossentropy',\n        optimizer = 'adam' , metrics = ['acc']\n        \n    )\n \nmodel.summary()    \n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"BMj7z3ldvm2o","trusted":true},"cell_type":"code","source":"## convert y_train into one hot encoder\nfrom sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\nencoder.fit(y_train)\ny_train = encoder.transform(y_train)\n# convert integers to dummy variables (i.e. one hot encoded)\ny_train = np_utils.to_categorical(y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## convert y_train into one hot encoder\nfrom sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\nencoder.fit(y_test)\ny_test = encoder.transform(y_test)\n# convert integers to dummy variables (i.e. one hot encoded)\ny_test = np_utils.to_categorical(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test[54]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train[3333]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"TXCk16Axvm2v"},"cell_type":"markdown","source":" "},{"metadata":{"id":"C8uJIs6Svm2x"},"cell_type":"markdown","source":"### Task 6: Train and Evaluate Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"steps_per_epoch = X_train.shape[0]/batch_size\nsteps_per_epochs_validation = X_test.shape[0]/batch_size\n\nsteps_per_epochs_validation","execution_count":null,"outputs":[]},{"metadata":{"id":"LE1ldSAZvm2y","trusted":true},"cell_type":"code","source":"epochs = 15\nsteps_per_epoch = X_train.shape[0]/batch_size\nsteps_per_epochs_validation = X_test.shape[0]/batch_size\n\nCheckpoint = ModelCheckpoint('model.h5', \n                        save_best_only=True ,\n                        save_weights_only = False ,\n                        mode = 'max',\n                        monitor = 'val_accuracy')\n\nreduce_lr = ReduceLROnPlateau(monitor = 'val_loss' , \n                              factor = 0.1 , \n                              patience = 2 , \n                              min_lr = 0.00001 ,\n                              mode = 'auto' )\n\n \n\ncallbacks = [Checkpoint , reduce_lr , EarlyStopping(monitor='val_accuracy')]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nhistory = model.fit(X_train,     \n                    y_train, \n                    steps_per_epoch = steps_per_epoch,\n                    epochs=25,\n                    validation_data=(X_test,y_test),   \n                    validation_steps = steps_per_epochs_validation,\n                    callbacks=callbacks\n                   \n           \n            )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('./fer2013/model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## test model using test data\n\ntest = pd.read_csv('../input/challenges-in-representation-learning-facial-expression-recognition-challenge/test.csv')\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open (\"../input/challenges-in-representation-learning-facial-expression-recognition-challenge/test.csv\", 'r') as f:\n    data1=[]\n    flag = True\n    reader = csv.reader (f, delimiter=',')\n    for row in reader:\n        \n    # skip first row\n        if flag == True:\n            flag = False\n\n        else:\n            \n            data1.append([int(p) for p in row[0].split()])\n \n\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = np.array(data1)\n\nsample,w = data1.shape\n\ndata1 = data1.reshape(sample , 48 ,48 ,1)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = data1/255.0","execution_count":null,"outputs":[]},{"metadata":{"id":"E-qp2iqOvm24"},"cell_type":"markdown","source":" "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model.predict(data1)\nnp.argmax(test_pred[5])","execution_count":null,"outputs":[]},{"metadata":{"id":"AhgCGFVbvm25"},"cell_type":"markdown","source":"### Task 7: Represent Model as JSON String"},{"metadata":{"id":"cHw8ir7CVAE0","trusted":true},"cell_type":"code","source":"model_json = model.to_json()\n\nwith open(\"model.json\",\"w\") as json_file:\n    json_file.write(model_json)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# green label is correct \n# red_label is not correct\n\n\n# visualize some of image\nplt.figure(figsize = (20,10))\nfor i in range(10):\n    plt.subplot(2,5,i+1)\n    index = np.random.choice(X_test.shape[0] , 1 , replace =False)\n    img = X_test[index].reshape(48,48)\n    img = rgb2gray(img)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n\n    if np.argmax(test_pred[index]) == np.argmax(y_test[index]):\n        plt.xlabel(class_name(np.argmax(test_pred[index]))  , color = 'green')\n    else:\n        plt.xlabel(class_name(np.argmax(test_pred[index]))  , color = 'red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,15))\naccs = history.history['acc']\nval_acc = history.history['val_acc']\n\nplt.plot(range(len(accs)) , accs , label = 'trainning')\nplt.plot(range(len(accs)) , val_acc , label = 'validation')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\nmatrix = metrics.confusion_matrix(np.argmax(y_test , axis =1), np.argmax(test_pred , axis =1))\nmatrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.argmax(data1[787])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualization on test data\n\n\n# visualize some of image\nplt.figure(figsize = (20,10))\nfor i in range(10):\n    plt.subplot(2,5,i+1)\n    index = np.random.choice(data1.shape[0] , 1 , replace =False)\n    img = data1[index].reshape(48,48)\n    img = rgb2gray(img)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n\n    plt.xlabel(class_name(np.argmax(test_pred[index]))  , color = 'green')\n    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}